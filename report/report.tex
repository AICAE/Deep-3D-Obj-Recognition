\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\RequirePackage{framed}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the 3DV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{3D Object Recognition with Deep Networks}

\author{Adrian Schneuwly\\
{\tt\small adrischn@ethz.ch}
\and
Johannes Oswald\\
{\tt\small voswaldj@eth.ch}
\and
Tobias Grundmann\\
{\tt\small tobiagru@ethz.ch}
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
 
 %The increasing amount of LiDAR and RGBD cameras in e.g. mobile devices
 %  and robotic systems provide us with new information which can aid in various task including object
 %  recognition. In this paper we will discribe 3D Convolutional Deep Neural Networks and  how they are
 %  trained to recognize voxelized point cloud data from common objects.
   
  We implement 3D object recognition using a Convolutional Neural Network (CNN) approach. We adopt the network structure 
  outlined in \cite{voxnet} and implement it in Python using the Keras framework. We compare our trained CNN
  with the original paper and achieve similar positive results.

\end{abstract}
%%%%%%%%% BODY TEXT

\section{Introduction}

%One of the crucial tasks of computer systems based on visual information e.g. self-driving cars, autonomous robots, virtual
%enviroments ( Figure \ref{fig:obj_rec}) is to get a semantic understanding of the enviroment. Depth data has already proved its
%usability in obstacle avoidance or mapping but we want to investigate its potential to improve object recognition.

%Since Deep Learning dramatically improved state of the art object recognition \cite{cnn}, we will use Neural Networks to
%recognize objects from given point clouds. Especially the idea behind convolution in 2D Object recognition will be translated to our 
%3D data, resulting in a fast and accurate detector. 

Autonomous robotic systems like self-driving cars or drones operating in real world environments strongly depend on robust object recognition
in order to get a semantic understanding of the immediate surroundings.
Meanwhile, inexpensive depth data has become available with active range sensor such as LiDAR and RGBG cameras (e.g Google Tango). 
This information can aid in a wide range of tasks. While it is commonly used for obstacle avoidance and mapping, it is not fully 
utilized in the task of object recognition.


Machine learning approaches (Fig. \ref{fig:obj_rec}) in the form of Convolutional Neural Networks (CNN) dramatically improved state 
of the art image recognition \cite{krizhevsky2012imagenet}. Such networks learn the feature and classifiers from given training data.
While it is simple to extend this basic approach to volumetric data, it is not obvious what network structures will yield good performance.
VoxNet \cite{voxnet} is a fast basic 3D CNN architecture that makes use of the available depth information and achieves high object detection accuracy for 
3D point clouds.


\begin{figure}[h]
	\label{fig:obj_rec}
	\centering
	\frame{\includegraphics[width=0.45\textwidth]{figures/obj_rec}}
	\caption{Example of object recognitions: Detection as binary classification problem (pedestrian vs no pedestrian). Slide over all possible locations in the image and classify patches. 
	  (Image source: \cite{udacity})}
\end{figure}

%-------------------------------------------------------------------------

\section{Related Work}

This section is strongly based on the 'Related Work' section of VoxNet paper \cite{voxnet}.
%On the one hand, there is a large body of work of object recognition using 3D point clouds but not making use of Neural Networks. 
%Mostly methods combine hand-crafted features or descriptors with a machine learning classifier ([10], [11], [12]). 
%Similiar methods are seen with semantic segmentation, with structured output classifiers instead of single output classifiers ([14], [15], [16]).

%On the other hand 2.5D CNNs were used for object recognition but fail to make full use of geometric information in the data. T
%heir approaches simply treat the depth channel as an additional channel, along with the RGB channels. ([17], [18], [19], [20]) 

%Additionally, 3D CNNs already proved there usability in video analysis ([23], [24]). In this case, time acts as the third dimension but algorithmically, 
%these architectures work the same as ours, but the nature of the data is very different.


\subsection{Object Recognition with Point Cloud Data}
Various publications on both semantic segmentation \cite{koppula2011semantic} and object recognition 
(\cite{teichman2011towards}, \cite{golovinskiy2009shape}) using 3D point clouds from LiDAR or RGBD sensors are available. 
Most of this work uses a pipeline combining hand-crafted features or descriptors with a machine learning classifier.
The VoxNet architecture on the other hand learns to extract features and classify objects from the raw volumetric data.


\subsection{3D  Convolutional Neural Networks}
RBG image object recognition has been dramatically improved by Convolutional Neural Networks.
Building on that success, several authors have extended that basic approach to RBGD, simply treating the depth as 
an additional channel (\cite{hoft2014fast}, \cite{socher2012convolutional}). While straightforward, this approach does not
make full use of the geometric information available in the data.
In contrast, the fully volumetric representation described in \cite{voxnet} takes advantage of that information.
This volumetric representation is richer as it also distinguishes free space from unknown space.

Additionally, CNN architectures with volumetric (3D) convolutions have already proven their usability in video analysis \cite{ji20133d}. In this
case time acts as the third dimension. 
In the RBGD domain, \cite{shape} proposes ``a convolutional deep belief network to represent geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid.''
Their model can jointly recognize and reconstruct objects from a single view. In order to train the deep belief network the authors constructed the 
3D CAD model dataset called ModelNet (see section \ref{data:modelnet}).



\section{Convolutional Neural Networks}

At the core of a neural network is a logistic classifier. 
%For given data, we train weights and bias of a linear equation which will then be transformed into 
%probabilities by using a softmax function. Figure \ref{fig:classifier}.
Weights and bias of a linear equation are trained on provided data. Given an input to the system the output is
transformed into probabilities by using a softmax function. See Figure \ref{fig:classifier}.


\begin{figure}[h]
	\label{fig:classifier}
	\centering
	\includegraphics[width=0.45\textwidth]{figures/classifier}
	\caption{Weights W and bias b are trained to classify an input. If the letter 'A' is feed to the network,
	it is classified as an 'a' with a certainty of 0.7. (Image source: \cite{udacity})}
\end{figure}

Linear models are limited and therefore we deepen the network in our case through convolutions and max pooling.
For convolutions, we run small neural networks with given output size/ depth on a patch which walks through our 3D Data. When leaving out /jumping voxels (striding),
the spatial dimension is squeezed while extracting local features of the given object. 
Through repeating this procedure we arrive at an output depth which corresponds to the semantic complexity of our object recognition task. 

\begin{figure}[h]
	\label{fig:convolution}
	\includegraphics[width=0.45\textwidth]{figures/conv}
	\caption{(a): Patch with stride 2 (b): Multiple Convolutions. (Image source: \cite{udacity})}
\end{figure}

The second technique to deepen our network is pooling. Instead of striding to squeeze our dimension, we can extract e.g. the maximum of a patch and therefore reduce spatial dimension. 
See detailed description of model used in our approach in Figure \ref{fig:model}.

\begin{figure}[h]
	\label{fig:pooling}
	\includegraphics[width=0.25\textwidth]{figures/con_max}
	\includegraphics[width=0.16\textwidth]{figures/max}
	\caption{Left: Stride 2 vs. Stride 1 + Pooling \quad Right: Max-Pooling. Source: \cite{udacity}}
\end{figure}

In order to train our model, we have to test how well it is doing during and after training. 
Validating the model on training data does not work since the model remembers all its input. Therefore we split the data into test- and training set which enables us to check how accurate the classifier is doing on unknown input, the test set. This common phenomenon, that a complex model fits a given dataset but fails to generalizes on new data, is called \textit{overfitting}.

\section{Datasets}
\label{data:modelnet}

In order for a CNN to perform well, the amount of training data needs to be sufficiently large. One large scale 3D CAD dataset is Princeton's 
ModelNet \cite{shape}. The dataset was constructed by querying common object categories from the SUN database \cite{sun} and 
by collecting 3D CAD models through online search engines. Human workers on Amazon Mechanical Turk were hired 
to manually validate the class labels assigned to each object.

The subset ModelNet40 is a collection of the 40 most common object categories with 100 unique CAD models each. The data is further augmented
by rotating each model by 30 degrees along the gravity direction (12 poses per model). 
The ModelNet40 data is available online in a voxelized version.  The grid size is 30x30x30 with 1 extra cell of padding in both directions.

\section{Implementation: VoxNet \cite{voxnet}}

\textit{In the following, parts contributed by author Adrian/Johannes/Tobias will be labeled 
as A1/A2/A3.}\newline

The overall goal of the project is to classify an object from a given 3D point cloud. Figure \ref{fig:algo} outlines 
the required pipeline.

\begin{figure}[h]
	\label{fig:algo}
	\centering
	\includegraphics[width=0.5\textwidth]{figures/algo}
	\caption{ Depth and point cloud are converted to voxels and then feed to the network 
	for classification  (Source: \cite{shape})}
\end{figure}

The voxelized version of ModelNet40 is provided in the form of Matlabs (.mat) files, where every single CAD model is stored 
in a separate file.

In a first step, A3 converted the data to the hdf5 format and stored all the model data in a single file in order to overcome 
the cumbersome character of the file structure. The Hdf5 format not only compresses the data size dramatically, but also enables
very simple data access. 
Next, A2/A3 implemented (in Python) a model data handling library, which can load the voxel grids from the hdf5. Generally, during the loading 
process the model set was split into two parts, a training and validation set. The responsibility of the library is to prepare 
the voxels arrays for the CNN, including shuffling the data and spliting it into batches.


%Two methods were implemented by A2/A3 to deal with loading the data from .hdf5 files.
%One opens the .hdf5 and loads pieces which are passed during iterations of the generator resulting 
%in a slower but RAM saving loader. The other, used for the final training, converts the hdf5 entirely to a numpy array 
%providing us with a RAM expensive but much faster access to the data.

The second important part, is the Convolutinal Neural Network itself, which was implemented by A1/A2. in Python
using the highly modular deep learning library \textit{Keras} on top of Theano.

Tensorflow, which at this point does not support 3D convolutions.


Figure \ref{fig:model}. For this, which runs on top of \textit{Theano}. 
\textit{Keras} supports 3D convolutional and max-pooling layers. 

After the conversion to a voxel grid of format 32x32x32, we train our CNN on these occupancy grids, Figure \ref{fig:algo}.





\begin{figure}[h]
	\label{fig:model}
	\centering
	\includegraphics[width=0.42\textwidth]{figures/model}
	\caption{VoxNet Convolution Model. (Source: \cite{mature}}
\end{figure}

A training enviroment was set up and used by A1/A2/A3. and training
we restrict

For our experiments we mostly done/trained with modelnet40.


\section{Training}

The training process takes around 9 to 20 hours on a NVIDIA GTX 980TI (6GB) GPU, using cuda 7.5 and cudnn 5, depending on the choice of the dataset(ModelNet10/40). Any methods to improve the score by further data augmentation, such as the random augmentation and multi resolution where not implemented as the amount of gpu time available was very limited. This might affect the results of the score. 
The implementation does not make use of early stopping as we are using droupout and weight regulizers, thus avoiding overfitting by the use of early stopping, would not show any improvements. This was tested and proven to be true. Any attampts of improving the score by Tuning the learning rate have not show to be successful.
Common to best practice weights are saved after every epoche of training to avoid loss of data. \textbf{TODO: Mehr \& Warum?} \\ 

\section{Results}

The papers implementation achieved verg good results for the small data set but failed to achieve the excellent score of the 
original VoxNet implementation on the larger ModelNet40. 
This is due ... \textbf{TODO: Mehr \& Warum?} \\ 
\begin{center}
\begin{tabular}{ |p{2.3cm}||p{2.3cm}|p{2.3cm}|  }
 \hline
 Algorithm & ModelNet10 Classification Accuracy  & ModelNet40 Classification Accuracy \\
 \hline
 VoxNet \cite{voxnet}   & 83\% & 92\% \\
 3DShapeNets  \cite{shape}   & 77 \% & 83.5\% \\
\textbf{ETH VoxNet}    & \textbf{81.8\%}   & \textbf{82.3 \%}  \\
 \hline
\end{tabular}
\end{center}{center}

\vspace{0.3cm}
A1/A2/A3 also tested the network on data aquired with google tango. The point clouds were first voxelized
and shaped into the right format to feed into the network. We feed voxels of 3 different objects/scenes, chair, 
stool (Fig. \ref{fig:voxel_stool}) and a desk scene. Single objects e.g. chair, stool get recognized with very high accuracies of 98\% or more.

%TODO image r√§nder abschneiden (weniger weiss)
\begin{figure}[h]
	\label{fig:voxel_stool}
	\centering
	\includegraphics[width=0.42\textwidth]{figures/tango_voxel_stool}
	\caption{Voxaliced stool \textbf{ source ????? martin ?}}
\end{figure}


\begin{figure}[h]
	\label{fig:voxel_desk}
	\centering
	\includegraphics[width=0.42\textwidth]{figures/tango_voxel_desk_scene}
	\caption{Voxaliced desk \textbf{source ????? martin ?}}
\end{figure}

Not surprisingly, the systems is designed to distinguish between single object and therefore recognizes a bed with high accuracy when
tested on the desk scene (chair, desk, display, etc.).

\section{Conclusion}

The team successfully reimplemented a very clean and working VoxNet with comparable results.
Improvements of results 
\textbf{TODO: Mehr...}

{\small
\bibliographystyle{ieee}
\bibliography{reference}
}

\end{document}
